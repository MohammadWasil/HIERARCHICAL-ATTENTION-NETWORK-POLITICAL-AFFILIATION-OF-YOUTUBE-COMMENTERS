{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import re, os\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = get_tokenizer('basic_english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "class CommentDataset(Dataset):\n",
    "    def __init__(self, folder_path, train=False, test=False, valid=False):\n",
    "        \n",
    "        if (train==False and test==False and valid==False):\n",
    "            raise Exception('One of the `train`, `test` or `valid` needs to be True, got `train = {}` `test = {}` and `valid = {}`'.format(train, test, valid))\n",
    "        if (train==True and test==True and valid == True):\n",
    "            raise Exception('Only one of the `train`, `test` or `valid` needs to be True, got `train = {}` `test = {}` and `valid = {}`'.format(train, test, valid))\n",
    "        if (train==True and test==True):\n",
    "            raise Exception('Only one of the `train` or `test` needs to be True, got `train = {}`, and `test = {}`'.format(train, test))\n",
    "        if (train==True and valid==True):\n",
    "            raise Exception('Only one of the `train` or `valid` needs to be True, got `train = {}`, and `valid = {}`'.format(train, valid))\n",
    "        if (test==True and valid==True):\n",
    "            raise Exception('Only one of the `test` or `valid` needs to be True, got `test = {}`, and `valid = {}`'.format(test, valid))\n",
    "\n",
    "        self.train_df = None\n",
    "        self.test_df = None\n",
    "        self.valid_df = None\n",
    "\n",
    "        # boolean values\n",
    "        self.train = train\n",
    "        self.test = test\n",
    "        self.valid = valid\n",
    "\n",
    "        self.data_selected = None\n",
    "        self.comment_selected = None\n",
    "    \n",
    "        self.train_comment = []\n",
    "        self.test_comment = []\n",
    "        self.val_comment = []\n",
    "    \n",
    "        # Read the dataset\n",
    "        self.data = pd.read_csv(folder_path, sep = \",\")#.head(20)\n",
    "        \n",
    "        self.data = shuffle(self.data)\n",
    "        self.data.reset_index(inplace=True, drop=True)\n",
    "        \n",
    "        self.data['Annot'].replace('LEFT', 0, inplace=True)\n",
    "        self.data['Annot'].replace('RIGHT', 1, inplace=True)\n",
    "        \n",
    "        # split the dataset into train, test, and valid.\n",
    "        self.train_df, test_df = train_test_split(self.data, test_size=0.2,  random_state=11)\n",
    "        self.test_df, self.valid_df = train_test_split(test_df, test_size=0.5,  random_state=96)\n",
    "        \n",
    "        if self.train == True:\n",
    "            # do the sorting\n",
    "            # Sort the dataframe according to the number of comments on documents.\n",
    "            self.train_df.sort_values(by=['Number of Comment'], ascending=False, inplace=True)       \n",
    "            comments = []\n",
    "            for com in self.train_df[\"comment\"]:\n",
    "                comments.append(com.replace(\" -|- \", \".\"))\n",
    "            self.train_comment = comments\n",
    "            \n",
    "        elif self.test == True:\n",
    "            # no need to sort\n",
    "            comments = []\n",
    "            for com in self.test_df[\"comment\"]:\n",
    "                comments.append(com.replace(\" -|- \", \".\"))\n",
    "            self.test_comment = comments\n",
    "        elif self.valid == True:\n",
    "            # no need to sort\n",
    "            comments = []\n",
    "            for com in self.valid_df[\"comment\"]:\n",
    "                comments.append(com.replace(\" -|- \", \".\"))\n",
    "            self.val_comment = comments\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.train == True:\n",
    "            self.data_selected = self.train_df\n",
    "            self.comment_selected = self.train_comment\n",
    "\n",
    "        elif self.test == True:\n",
    "            self.data_selected = self.test_df\n",
    "            self.comment_selected = self.test_comment\n",
    "\n",
    "        elif self.valid == True:\n",
    "            self.data_selected = self.valid_df\n",
    "            self.comment_selected = self.val_comment\n",
    "\n",
    "        label = self.data_selected.iloc[idx][\"Annot\"]\n",
    "        sentence = self.comment_selected[idx]\n",
    "        return sentence, label\n",
    "\n",
    "    def __len__(self):\n",
    "        \n",
    "        \n",
    "        if self.train == True:\n",
    "            len_ = len(self.train_comment)\n",
    "\n",
    "        elif self.test == True:\n",
    "            len_ = len(self.test_comment)\n",
    "\n",
    "        elif self.valid == True:\n",
    "            len_ = len(self.val_comment)\n",
    "\n",
    "        return len_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = \"12. Subscription Training Data.csv\"\n",
    "\n",
    "dataset_train = CommentDataset(data_folder, train=True, test=False, valid=False)\n",
    "dataset_test = CommentDataset(data_folder, train=False, test=True, valid=False)\n",
    "dataset_valid = CommentDataset(data_folder, train=False, test=False, valid=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yield_tokens(data_iter):\n",
    "    for sentence, _ in data_iter:\n",
    "        yield tokenizer(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create vocabulary from the training data.\n",
    "vocab = build_vocab_from_iterator(yield_tokens(dataset_train), specials=[\"<unk>\"])\n",
    "vocab.set_default_index(vocab[\"<unk>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32822"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = len(vocab.get_itos()) # len(vocab.get_stoi()) - length of the vocabulary\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_pipeline = lambda x: vocab(tokenizer(x))\n",
    "label_pipeline = lambda x: int(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# works perfectly. backup\n",
    "def collate_batch(batch):\n",
    "    label_list, text_list = [], []\n",
    "    for (_text, _label) in batch:\n",
    "        label_list.append(torch.tensor(label_pipeline(_label), dtype=torch.int64 ))\n",
    "        processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64)\n",
    "        text_list.append(processed_text)\n",
    "    \n",
    "    text_list = pad_sequence(text_list, batch_first=False, padding_value = vocab_size-1)\n",
    "   \n",
    "    # convert a list of tensors to tensors.\n",
    "    # input : a list of tensors of len BATCH_SIZE\n",
    "    label_list = torch.stack(label_list)   \n",
    "    # OUTPUT shape: [BATCH_SIZE]\n",
    "    \n",
    "    return text_list.to(device), label_list.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BATCH SAMPLER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.sampler import BatchSampler\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.sampler import BatchSampler\n",
    "import numpy as np\n",
    "\n",
    "class YoutubeBatchSampler(BatchSampler):\n",
    "    def __init__(self, dataset, num_of_liberals, num_of_conservatives):\n",
    "        \n",
    "        self.label_list = []\n",
    "        for _, label in dataset:\n",
    "            self.label_list.append(label)\n",
    "\n",
    "        self.label_list = torch.LongTensor(self.label_list) # list of all the labels in the dataset\n",
    "        \n",
    "        self.label_set = list(set(self.label_list.numpy())) # unique labels from the dataset\n",
    "\n",
    "        self.label_to_indices = {label: np.where(self.label_list.numpy() == label)[0]\n",
    "                                 for label in self.label_set}\n",
    "\n",
    "        for l in self.label_set:\n",
    "            np.random.shuffle(self.label_to_indices[l])\n",
    "\n",
    "        self.used_label_indices_count = {label: 0 for label in self.label_set}\n",
    "        self.count = 0\n",
    "        self.dataset = dataset\n",
    "        self.num_of_liberals = num_of_liberals\n",
    "        self.num_of_conservatives = num_of_conservatives\n",
    "        self.batch_size = self.num_of_liberals + self.num_of_conservatives\n",
    "\n",
    "    def __iter__(self):\n",
    "        self.count = 0\n",
    "        \n",
    "        # maybe add <= \n",
    "        while self.count + self.batch_size < len(self.dataset):\n",
    "            classes = np.array([1, 0])\n",
    "            indices = []\n",
    "            for class_ in classes:\n",
    "                if(class_ == 0) :\n",
    "                    indices.extend(self.label_to_indices[class_][self.used_label_indices_count[class_] : self.used_label_indices_count[class_] + self.num_of_liberals])\n",
    "                    self.used_label_indices_count[class_] += self.num_of_liberals\n",
    "\n",
    "                    if self.used_label_indices_count[class_] + self.num_of_liberals > len(self.label_to_indices[class_]):\n",
    "                        np.random.shuffle(self.label_to_indices[class_])\n",
    "                        self.used_label_indices_count[class_] = 0\n",
    "              \n",
    "                elif(class_ == 1):\n",
    "                    indices.extend(self.label_to_indices[class_][self.used_label_indices_count[class_] : self.used_label_indices_count[class_] + self.num_of_conservatives])\n",
    "                    self.used_label_indices_count[class_] += self.num_of_conservatives\n",
    "\n",
    "                    if self.used_label_indices_count[class_] + self.num_of_conservatives > len(self.label_to_indices[class_]):\n",
    "                        np.random.shuffle(self.label_to_indices[class_])\n",
    "                        self.used_label_indices_count[class_] = 0\n",
    "              \n",
    "              \n",
    "            yield indices\n",
    "            self.count = self.count + self.num_of_conservatives + self.num_of_liberals\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.dataset) // self.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "#num_of_liberals = int(BATCH_SIZE / 2)\n",
    "#num_of_conservatives = int(BATCH_SIZE - num_of_liberals)\n",
    "#batch_sampler_train = YoutubeBatchSampler(dataset_train, num_of_liberals, num_of_conservatives)\n",
    "train_dataloader = DataLoader(dataset_train, batch_size=BATCH_SIZE,\n",
    "                              shuffle=False, collate_fn=collate_batch)\n",
    "val_dataloader = DataLoader(dataset_valid,\n",
    "                              shuffle=False, collate_fn=collate_batch)\n",
    "test_dataloader = DataLoader(dataset_test,\n",
    "                              shuffle=False, collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UserClassificationModel(nn.Module):\n",
    "\n",
    "    def __init__(self, VOCAB_SIZE, EMBEDDING_DIMENSION, num_class, ENCODER_HIDDEN_DIMENSION, embedding_matrix):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.vocab_size = VOCAB_SIZE\n",
    "        self.embed_dim = EMBEDDING_DIMENSION\n",
    "        \n",
    "        self.encoder_hidden_dim = ENCODER_HIDDEN_DIMENSION\n",
    "        \n",
    "        self.num_class = num_class\n",
    "\n",
    "        if USE_PRETRAINED_EMBEDDING_MATRIX:\n",
    "            self.vocab_size = embedding_matrix.shape[0]\n",
    "            self.embed_dim = embedding_matrix.shape[1]\n",
    "            \n",
    "            self.embedding = nn.Embedding(num_embeddings = self.vocab_size, embedding_dim = self.embed_dim)\n",
    "            self.embedding.weight=nn.Parameter(torch.tensor(embedding_matrix, dtype=torch.float32))\n",
    "        else:\n",
    "            self.embedding = nn.Embedding(self.vocab_size, self.embed_dim)\n",
    "        \n",
    "        self.gru = nn.GRU(self.embed_dim, self.encoder_hidden_dim, bidirectional = True)\n",
    "        \n",
    "        self.fc1 = nn.Linear(self.encoder_hidden_dim*2, self.num_class)\n",
    "        \n",
    "    def forward(self, text):\n",
    "        \n",
    "        # input shape: [MAX_LENGTH_OF_THE_SENTENCE_IN_BATCH X BATCH_SIZE] \n",
    "        embedded = self.embedding(text)\n",
    "        # output shape: [MAX_SEQ_LENGTH x BATCH_SIZE x embed_dim]\n",
    "        \n",
    "        # input shape: [MAX_SEQ_LENGTH x BATCH_SIZE x embed_dim]\n",
    "        gru_out, hidden = self.gru(embedded)\n",
    "        # gru_out shape: [MAX_LENGTH_OF_THE_SENTENCE_IN_BATCH, BATCH_SIZE, ENCODER_HIDDEN_DIMENSION*2]\n",
    "        # hidden[0] shape: [1, BATCH_SIZE, ENCODER_HIDDEN_DIMENSION]\n",
    "        # hidden[1] shape: [1, NUM_SENTENCES, ENCODER_HIDDEN_DIMENSION]\n",
    "        \n",
    "        # input shape: [BATCH_SIZE, ENCODER_HIDDEN_DIMENSION*2]\n",
    "        fc1 = self.fc1(gru_out[-1])\n",
    "        # output shape: [BATCH_SIZE, num_classes]\n",
    "        \n",
    "        # input shape: [BATCH_SIZE, Num_class]\n",
    "        classifier = F.softmax(fc1, dim=1)#.squeeze(1)\n",
    "        # output shape: [BATCH_SIZE, Num_class]\n",
    "        \n",
    "        return classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Download_and_extract():\n",
    "    print(\"This might take some time...\")\n",
    "    print(\"Downloading...\")\n",
    "    os.system('wget https://nlp.stanford.edu/data/glove.840B.300d.zip')\n",
    "    \n",
    "    Extract()\n",
    "    \n",
    "def Extract():\n",
    "    print(\"Extracting...\")\n",
    "    # extract and save to the same directory.\n",
    "    with zipfile.ZipFile('glove.840B.300d.zip', 'r') as zip_ref:\n",
    "        zip_ref.extractall(\"./\")\n",
    "    print(\"Done!\")\n",
    "    \n",
    "def load_pretrained_embedding_matrix():\n",
    "    # Downloadin Glove word vector\n",
    "    # this might take some time........... ~5 mins.\n",
    "    if((os.path.isfile('glove.840B.300d.zip') == False)):\n",
    "        Download_and_extract()\n",
    "    elif((os.path.isfile('glove.840B.300d.zip') == True) and (os.path.isfile('glove.840B.300d.txt') == False)):\n",
    "         Extract()\n",
    "    else:\n",
    "        print(\"Already Downloaded and extracted!\")\n",
    "\n",
    "    #!wget https://nlp.stanford.edu/data/glove.840B.300d.zip\n",
    "    #!unzip glove.840B.300d.zip\n",
    "\n",
    "# https://github.com/MohammadWasil/Visual-Question-Answering-VQA/blob/master/2.%20Dataset%20Used%20in%20Training..ipynb\n",
    "def GloveModel(file_path, vocab):\n",
    "    embedding_index = {}\n",
    "    f = open(file_path,'r', encoding='utf8')\n",
    "    embedding_index = {}\n",
    "    print(\"Opened!\")\n",
    "\n",
    "    for j, line in enumerate(f):\n",
    "        splitLine = line.split(' ')\n",
    "        word = splitLine[0]\n",
    "        embedding = np.asarray(splitLine[1:], dtype='float32')\n",
    "        embedding_index[word] = embedding\n",
    "      \n",
    "    print(\"Done.\",len(embedding_index),\" words loaded!\")\n",
    "    EMBEDDING_DIM = 300\n",
    "    embedding_matrix = np.zeros((len(vocab.get_stoi()) + 1, EMBEDDING_DIM))\n",
    "    print(embedding_matrix.shape)\n",
    "\n",
    "    for index, word in enumerate(vocab.get_itos()):\n",
    "        embedding_vector = embedding_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "          # words not found in embedding index will be all-zeros.\n",
    "          embedding_matrix[index] = embedding_vector\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# can change this accordingly.\n",
    "USE_PRETRAINED_EMBEDDING_MATRIX = False\n",
    "\n",
    "if USE_PRETRAINED_EMBEDDING_MATRIX:\n",
    "    # download an dextract the glove embedding if they're not.\n",
    "    load_pretrained_embedding_matrix()\n",
    "    # load the embedding matrix.\n",
    "    embedding_matrix = GloveModel(\"glove.840B.300d.txt\", vocab)\n",
    "else:\n",
    "    embedding_matrix = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_class = 2\n",
    "embedding_size = 100\n",
    "ENCODER_HIDDEN_DIMENSION = 64\n",
    "\n",
    "model = UserClassificationModel(vocab_size, embedding_size, num_class, ENCODER_HIDDEN_DIMENSION, embedding_matrix).to(device)\n",
    "\n",
    "def train(model, train_dataloader):\n",
    "    optimizer = Adam(model.parameters(), 0.0001)\n",
    "    loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "    train_loss_list = []\n",
    "    val_loss_list = []\n",
    "    train_accu_list = []\n",
    "    val_accu_list = []\n",
    "\n",
    "    for epoch in range(0, 3):\n",
    "        model.train()\n",
    "        model.to(device)\n",
    "        train_loss = 0\n",
    "        \n",
    "        accuracy = 0\n",
    "        \n",
    "        for idx, (text, label) in enumerate(train_dataloader):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            predicted_label = model(text)\n",
    "            \n",
    "            loss = loss_function(predicted_label, label)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            #prediction = predicted_label#.argmax(1)#.item()\n",
    "            actual = label.reshape(-1)\n",
    "            \n",
    "            predicted_label = torch.argmax(predicted_label, dim=1 ) \n",
    "            accuracy += torch.eq(predicted_label, actual).sum().item()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "        train_loss = train_loss / len(train_dataloader)\n",
    "        accuracy = accuracy * 100.0 / len(dataset_train)\n",
    "\n",
    "        EPOCH_VAL_ACC, EPOCH_VAL_LOSS, F1_score = evaluate(val_dataloader, model, loss_function)\n",
    "\n",
    "        print(f'Epoch: {epoch+1} | Train Loss: {train_loss} | Accuracy: {accuracy} | Val Accuracy: {EPOCH_VAL_ACC} | Val Loss: {EPOCH_VAL_LOSS} | F1 Score: {F1_score}')\n",
    "        train_loss_list.append(train_loss)\n",
    "        val_loss_list.append(EPOCH_VAL_LOSS)\n",
    "        train_accu_list.append(accuracy)\n",
    "        val_accu_list.append(EPOCH_VAL_ACC)\n",
    "        \n",
    "    return train_loss_list, val_loss_list, train_accu_list, val_accu_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(val_dataloader, model, loss_function):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total_count = 0\n",
    "\n",
    "    # for f1 score\n",
    "    prediction_labels = []\n",
    "    actual_labels = []\n",
    "\n",
    "    val_loss = 0\n",
    "\n",
    "    with torch.no_grad():    \n",
    "        for text, label in val_dataloader:\n",
    "            text = text.to(device)\n",
    "            label = label.to(device)\n",
    "            \n",
    "            # feed the validation text into the model, and get the probabilities.\n",
    "            predicted_label = model(text)\n",
    "\n",
    "            # calculate loss\n",
    "            loss = loss_function(predicted_label, label)\n",
    "            \n",
    "            # validation accuracy\n",
    "            actual = label.reshape(-1)\n",
    "            predicted_label = torch.argmax(predicted_label, dim=1 ) \n",
    "            correct += torch.eq(predicted_label, actual).sum().item()\n",
    "\n",
    "            # to cal f1 score.\n",
    "            prediction_labels.append(predicted_label)\n",
    "            actual_labels.append(actual)   \n",
    "\n",
    "            # convert probabilities into 0/1.\n",
    "            #predicted_label = torch.round(predicted_label).type(torch.int64)\n",
    "            \n",
    "            # count the number of correctly predicted labels.\n",
    "            #correct += torch.eq(predicted_label, label).sum().item()\n",
    "            \n",
    "            # get the total length of the sentences in val_dataloader\n",
    "            #total_count += label.size(0)\n",
    "\n",
    "            val_loss += loss.item()\n",
    "        val_loss = val_loss / len(val_dataloader)\n",
    "\n",
    "        # convert unequal length of lists of tensors to on single tensors.\n",
    "        #actual_labels = torch.flatten(torch.stack(actual_labels)) \n",
    "        actual_labels = torch.cat(actual_labels).to('cpu')\n",
    "        #prediction_labels = torch.flatten(torch.stack(prediction_labels)) \n",
    "        prediction_labels = torch.cat(prediction_labels).to('cpu')\n",
    "        \n",
    "        F1_score = f1_score(actual_labels, prediction_labels)\n",
    "        \n",
    "    # returns the accuracy of the model\n",
    "    return correct * 100.0 / len(dataset_valid), val_loss, F1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1, 1,  ..., 1, 0, 0]) : tensor([0, 0, 0,  ..., 0, 0, 0])\n",
      "Epoch: 1 | Train Loss: 0.6692756467227694 | Accuracy: 60.59674861221253 | Val Accuracy: 62.044374009508715 | Val Loss: 0.6644521590599735 | F1 Score: 0.0\n",
      "tensor([0, 1, 1,  ..., 1, 0, 0]) : tensor([0, 0, 0,  ..., 0, 0, 0])\n",
      "Epoch: 2 | Train Loss: 0.6700526404229901 | Accuracy: 60.59674861221253 | Val Accuracy: 62.044374009508715 | Val Loss: 0.6638931219485218 | F1 Score: 0.0\n",
      "tensor([0, 1, 1,  ..., 1, 0, 0]) : tensor([0, 0, 0,  ..., 0, 0, 0])\n",
      "Epoch: 3 | Train Loss: 0.669942604024199 | Accuracy: 60.59674861221253 | Val Accuracy: 62.044374009508715 | Val Loss: 0.6640611524957862 | F1 Score: 0.0\n"
     ]
    }
   ],
   "source": [
    "train_loss_list, val_loss_list, train_accu_list, val_accu_list = train(han_model, train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The test accuracy is: 59.75%\n",
      "F1 Score on Test data is: 0.00\n",
      "Loss on Test Data is: 0.68\n"
     ]
    }
   ],
   "source": [
    "loss_function = nn.CrossEntropyLoss()\n",
    "TEST_ACC, TEST_LOSS, F1_score = evaluate(test_dataloader, han_model, loss_function = nn.CrossEntropyLoss())\n",
    "print(\"The test accuracy is: {:.2f}%\".format(TEST_ACC))\n",
    "print(\"F1 Score on Test data is: {:.2f}\".format(F1_score))\n",
    "print(\"Loss on Test Data is: {:.2f}\".format(TEST_LOSS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "x and y must have same first dimension, but have shapes (20,) and (3,)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-50-511361ebc00d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m# accuracy Plot\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mtrain_accu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0max1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m21\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_accu_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"Training Accuracy\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[0mval_accu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0max1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m21\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_accu_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"Validation Accuracy\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\matplotlib\\axes\\_axes.py\u001b[0m in \u001b[0;36mplot\u001b[1;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1741\u001b[0m         \"\"\"\n\u001b[0;32m   1742\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcbook\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnormalize_kwargs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmlines\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLine2D\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1743\u001b[1;33m         \u001b[0mlines\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_lines\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1744\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1745\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_line\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\matplotlib\\axes\\_base.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m    271\u001b[0m                 \u001b[0mthis\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    272\u001b[0m                 \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 273\u001b[1;33m             \u001b[1;32myield\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_plot_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mthis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    274\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    275\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_next_color\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\matplotlib\\axes\\_base.py\u001b[0m in \u001b[0;36m_plot_args\u001b[1;34m(self, tup, kwargs)\u001b[0m\n\u001b[0;32m    397\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    398\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 399\u001b[1;33m             raise ValueError(f\"x and y must have same first dimension, but \"\n\u001b[0m\u001b[0;32m    400\u001b[0m                              f\"have shapes {x.shape} and {y.shape}\")\n\u001b[0;32m    401\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: x and y must have same first dimension, but have shapes (20,) and (3,)"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x432 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "fig, (ax1) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "fig.suptitle('Loss and accuracy for HAN Model.')\n",
    "\n",
    "epochs = 30 + 1\n",
    "\n",
    "# accuracy Plot\n",
    "train_accu, = ax1[0].plot(range(1, epochs), train_accu_list, label=\"Training Accuracy\")  \n",
    "val_accu, = ax1[0].plot(range(1, epochs), val_accu_list, label=\"Validation Accuracy\")  \n",
    "\n",
    "ax1[0].legend(handles=[train_accu, val_accu])\n",
    "ax1[0].set_xlabel(\"Epochs\")\n",
    "ax1[0].set_ylabel(\"Accuracy\")\n",
    "ax1[0].set_title(\"Accuracy for every Epochs\")\n",
    "ax1[0].set_xticks(range(1, epochs))\n",
    "\n",
    "train_loss, = ax1[1].plot(range(1, epochs), train_loss_list, label=\"Training Loss\")  \n",
    "val_loss, = ax1[1].plot(range(1, epochs), val_loss_list, label=\"Validation Loss\")  \n",
    "\n",
    "ax1[1].legend(handles=[train_loss, val_loss])\n",
    "ax1[1].set_xlabel(\"Epochs\")\n",
    "ax1[1].set_ylabel(\"Loss\")\n",
    "ax1[1].set_title(\"Loss for every Epochs\")\n",
    "ax1[1].set_xticks(range(1, epochs))\n",
    "\n",
    "# do not need the third plot.\n",
    "#fig.delaxes(ax2[1])\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
