{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This notebook contains minimum implementation of LSTM model and training/evaluation functions.\n",
    "This notebook is/was not used for training/validating/inferencing for our research project.\n",
    "The implementation of LSTM model works for different datasets as well.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import re, os\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = get_tokenizer('basic_english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "class CommentDataset(Dataset):\n",
    "    def __init__(self, folder_path, train=False, test=False, valid=False):\n",
    "        \n",
    "        if (train==False and test==False and valid==False):\n",
    "            raise Exception('One of the `train`, `test` or `valid` needs to be True, got `train = {}` `test = {}` and `valid = {}`'.format(train, test, valid))\n",
    "        if (train==True and test==True and valid == True):\n",
    "            raise Exception('Only one of the `train`, `test` or `valid` needs to be True, got `train = {}` `test = {}` and `valid = {}`'.format(train, test, valid))\n",
    "        if (train==True and test==True):\n",
    "            raise Exception('Only one of the `train` or `test` needs to be True, got `train = {}`, and `test = {}`'.format(train, test))\n",
    "        if (train==True and valid==True):\n",
    "            raise Exception('Only one of the `train` or `valid` needs to be True, got `train = {}`, and `valid = {}`'.format(train, valid))\n",
    "        if (test==True and valid==True):\n",
    "            raise Exception('Only one of the `test` or `valid` needs to be True, got `test = {}`, and `valid = {}`'.format(test, valid))\n",
    "\n",
    "        self.train_df = None\n",
    "        self.test_df = None\n",
    "        self.valid_df = None\n",
    "\n",
    "        # boolean values\n",
    "        self.train = train\n",
    "        self.test = test\n",
    "        self.valid = valid\n",
    "\n",
    "        self.data_selected = None\n",
    "        self.comment_selected = None\n",
    "    \n",
    "        self.train_comment = []\n",
    "        self.test_comment = []\n",
    "        self.val_comment = []\n",
    "    \n",
    "        # Read the dataset\n",
    "        self.data = pd.read_csv(folder_path, sep = \",\")#.head(20)\n",
    "        \n",
    "        self.data = shuffle(self.data)\n",
    "        self.data.reset_index(inplace=True, drop=True)\n",
    "        \n",
    "        self.data['Authors Biasness'].replace('LEFT', 0, inplace=True)\n",
    "        self.data['Authors Biasness'].replace('RIGHT', 1, inplace=True)\n",
    "        \n",
    "        # split the dataset into train, test, and valid.\n",
    "        self.train_df, test_df = train_test_split(self.data, test_size=0.2,  random_state=11)\n",
    "        self.test_df, self.valid_df = train_test_split(test_df, test_size=0.5,  random_state=96)\n",
    "        \n",
    "        if self.train == True:\n",
    "            # do the sorting\n",
    "            # Sort the dataframe according to the number of comments on documents.\n",
    "            self.train_df.sort_values(by=['Num of Comments'], ascending=False, inplace=True)       \n",
    "            comments = []\n",
    "            for com in self.train_df[\"Authors Comment\"]:\n",
    "                comments.append(com.replace(\" -|- \", \".\"))\n",
    "            self.train_comment = comments\n",
    "            \n",
    "        elif self.test == True:\n",
    "            # no need to sort\n",
    "            comments = []\n",
    "            for com in self.test_df[\"Authors Comment\"]:\n",
    "                comments.append(com.replace(\" -|- \", \".\"))\n",
    "            self.test_comment = comments\n",
    "        elif self.valid == True:\n",
    "            # no need to sort\n",
    "            comments = []\n",
    "            for com in self.valid_df[\"Authors Comment\"]:\n",
    "                comments.append(com.replace(\" -|- \", \".\"))\n",
    "            self.val_comment = comments\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.train == True:\n",
    "            self.data_selected = self.train_df\n",
    "            self.comment_selected = self.train_comment\n",
    "\n",
    "        elif self.test == True:\n",
    "            self.data_selected = self.test_df\n",
    "            self.comment_selected = self.test_comment\n",
    "\n",
    "        elif self.valid == True:\n",
    "            self.data_selected = self.valid_df\n",
    "            self.comment_selected = self.val_comment\n",
    "\n",
    "        label = self.data_selected.iloc[idx][\"Authors Biasness\"]\n",
    "        sentence = self.comment_selected[idx]\n",
    "        return sentence, label\n",
    "\n",
    "    def __len__(self):\n",
    "        \n",
    "        \n",
    "        if self.train == True:\n",
    "            len_ = len(self.train_comment)\n",
    "\n",
    "        elif self.test == True:\n",
    "            len_ = len(self.test_comment)\n",
    "\n",
    "        elif self.valid == True:\n",
    "            len_ = len(self.val_comment)\n",
    "\n",
    "        return len_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = \"16. Training Dataset revisit.csv\"\n",
    "\n",
    "# create dataset loader for train and evaluation set.\n",
    "dataset_train = CommentDataset_LSTM(data_folder, train=True, test=False, valid=False)\n",
    "dataset_valid = CommentDataset_LSTM(data_folder, train=False, test=False, valid=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yield_tokens(data_iter):\n",
    "    for sentence, _ in data_iter:\n",
    "        yield tokenizer(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create vocabulary from the training data.\n",
    "vocab = build_vocab_from_iterator(yield_tokens(dataset_train), specials=[\"<unk>\"])\n",
    "vocab.set_default_index(vocab[\"<unk>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(vocab.get_itos()) # len(vocab.get_stoi()) - length of the vocabulary\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_pipeline = lambda x: vocab(tokenizer(x))\n",
    "label_pipeline = lambda x: int(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# works perfectly. backup\n",
    "def collate_batch(batch):\n",
    "    label_list, text_list = [], []\n",
    "    for (_text, _label) in batch:\n",
    "        label_list.append(torch.tensor(label_pipeline(_label), dtype=torch.int64 ))\n",
    "        processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64)\n",
    "        text_list.append(processed_text)\n",
    "    \n",
    "    text_list = pad_sequence(text_list, batch_first=False, padding_value = vocab_size-1)\n",
    "   \n",
    "    # convert a list of tensors to tensors.\n",
    "    # input : a list of tensors of len BATCH_SIZE\n",
    "    label_list = torch.stack(label_list)   \n",
    "    # OUTPUT shape: [BATCH_SIZE]\n",
    "    \n",
    "    return text_list, label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "train_dataloader = DataLoader(dataset_train, batch_size=BATCH_SIZE,\n",
    "                            shuffle=False, collate_fn=collate_batch_LSTM)\n",
    "val_dataloader = DataLoader(dataset_valid,\n",
    "                            shuffle=False, collate_fn=collate_batch_LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UserClassificationModel(nn.Module):\n",
    "\n",
    "    def __init__(self, VOCAB_SIZE, EMBEDDING_DIMENSION, num_class, ENCODER_HIDDEN_DIMENSION, USE_PRETRAINED_EMBEDDING_MATRIX, embedding_matrix):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.vocab_size = VOCAB_SIZE\n",
    "        self.embed_dim = EMBEDDING_DIMENSION\n",
    "        \n",
    "        self.encoder_hidden_dim = ENCODER_HIDDEN_DIMENSION\n",
    "        \n",
    "        self.num_class = num_class\n",
    "\n",
    "        if USE_PRETRAINED_EMBEDDING_MATRIX:\n",
    "            self.vocab_size = embedding_matrix.shape[0]\n",
    "            self.embed_dim = embedding_matrix.shape[1]\n",
    "            \n",
    "            self.embedding = nn.Embedding(num_embeddings = self.vocab_size, embedding_dim = self.embed_dim)\n",
    "            self.embedding.weight=nn.Parameter(torch.tensor(embedding_matrix, dtype=torch.float32))\n",
    "        else:\n",
    "            self.embedding = nn.Embedding(self.vocab_size, self.embed_dim)\n",
    "        \n",
    "        self.gru = nn.GRU(self.embed_dim, self.encoder_hidden_dim, bidirectional = True)\n",
    "        \n",
    "        self.fc1 = nn.Linear(self.encoder_hidden_dim*2, self.num_class)\n",
    "        \n",
    "    def forward(self, text):\n",
    "        \n",
    "        # input shape: [MAX_LENGTH_OF_THE_SENTENCE_IN_BATCH X BATCH_SIZE] \n",
    "        embedded = self.embedding(text)\n",
    "        # output shape: [MAX_SEQ_LENGTH x BATCH_SIZE x embed_dim]\n",
    "        \n",
    "        # input shape: [MAX_SEQ_LENGTH x BATCH_SIZE x embed_dim]\n",
    "        gru_out, hidden = self.gru(embedded)\n",
    "        # gru_out shape: [MAX_LENGTH_OF_THE_SENTENCE_IN_BATCH, BATCH_SIZE, ENCODER_HIDDEN_DIMENSION*2]\n",
    "        # hidden[0] shape: [1, BATCH_SIZE, ENCODER_HIDDEN_DIMENSION]\n",
    "        # hidden[1] shape: [1, NUM_SENTENCES, ENCODER_HIDDEN_DIMENSION]\n",
    "        \n",
    "        # input shape: [BATCH_SIZE, ENCODER_HIDDEN_DIMENSION*2]\n",
    "        fc1 = self.fc1(gru_out[-1])\n",
    "        # output shape: [BATCH_SIZE, num_classes]\n",
    "        \n",
    "        # input shape: [BATCH_SIZE, Num_class]\n",
    "        classifier = F.softmax(fc1, dim=1)#.squeeze(1)\n",
    "        # output shape: [BATCH_SIZE, Num_class]\n",
    "        \n",
    "        return classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Download_and_extract():\n",
    "    print(\"This might take some time...\")\n",
    "    print(\"Downloading...\")\n",
    "    os.system('wget https://nlp.stanford.edu/data/glove.840B.300d.zip')\n",
    "    \n",
    "    Extract()\n",
    "    \n",
    "def Extract():\n",
    "    print(\"Extracting...\")\n",
    "    # extract and save to the same directory.\n",
    "    with zipfile.ZipFile('glove.840B.300d.zip', 'r') as zip_ref:\n",
    "        zip_ref.extractall(\"./\")\n",
    "    print(\"Done!\")\n",
    "    \n",
    "def load_pretrained_embedding_matrix():\n",
    "    # Downloadin Glove word vector\n",
    "    # this might take some time........... ~5 mins.\n",
    "    if((os.path.isfile('glove.840B.300d.zip') == False)):\n",
    "        Download_and_extract()\n",
    "    elif((os.path.isfile('glove.840B.300d.zip') == True) and (os.path.isfile('glove.840B.300d.txt') == False)):\n",
    "         Extract()\n",
    "    else:\n",
    "        print(\"Already Downloaded and extracted!\")\n",
    "\n",
    "    #!wget https://nlp.stanford.edu/data/glove.840B.300d.zip\n",
    "    #!unzip glove.840B.300d.zip\n",
    "\n",
    "# https://github.com/MohammadWasil/Visual-Question-Answering-VQA/blob/master/2.%20Dataset%20Used%20in%20Training..ipynb\n",
    "def GloveModel(file_path, vocab):\n",
    "    embedding_index = {}\n",
    "    f = open(file_path,'r', encoding='utf8')\n",
    "    embedding_index = {}\n",
    "    print(\"Opened!\")\n",
    "\n",
    "    for j, line in enumerate(f):\n",
    "        splitLine = line.split(' ')\n",
    "        word = splitLine[0]\n",
    "        embedding = np.asarray(splitLine[1:], dtype='float32')\n",
    "        embedding_index[word] = embedding\n",
    "      \n",
    "    print(\"Done.\",len(embedding_index),\" words loaded!\")\n",
    "    EMBEDDING_DIM = 300\n",
    "    embedding_matrix = np.zeros((len(vocab.get_stoi()) + 1, EMBEDDING_DIM))\n",
    "    print(embedding_matrix.shape)\n",
    "\n",
    "    for index, word in enumerate(vocab.get_itos()):\n",
    "        embedding_vector = embedding_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "          # words not found in embedding index will be all-zeros.\n",
    "          embedding_matrix[index] = embedding_vector\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# can change this accordingly.\n",
    "USE_PRETRAINED_EMBEDDING_MATRIX = False\n",
    "\n",
    "if USE_PRETRAINED_EMBEDDING_MATRIX:\n",
    "    # download an dextract the glove embedding if they're not.\n",
    "    load_pretrained_embedding_matrix()\n",
    "    # load the embedding matrix.\n",
    "    embedding_matrix = GloveModel(\"glove.840B.300d.txt\", vocab)\n",
    "else:\n",
    "    embedding_matrix = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_class = 2\n",
    "embedding_size = 100\n",
    "ENCODER_HIDDEN_DIMENSION = 64\n",
    "\n",
    "model = UserClassificationModel(vocab_size, embedding_size, num_class, ENCODER_HIDDEN_DIMENSION, embedding_matrix).to(device)\n",
    "\n",
    "def train(model, train_dataloader, val_dataloader, dataset_train, dataset_valid):\n",
    "    optimizer = Adam(model.parameters(), 0.0001)\n",
    "    loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "    train_loss_list = []\n",
    "    val_loss_list = []\n",
    "    train_accu_list = []\n",
    "    val_accu_list = []\n",
    "\n",
    "    for epoch in range(0, 3):\n",
    "        model.train()\n",
    "        model.to(device)\n",
    "        train_loss = 0\n",
    "        \n",
    "        accuracy = 0\n",
    "        \n",
    "        for idx, (text, label) in enumerate(train_dataloader):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            predicted_label = model(text)\n",
    "            \n",
    "            loss = loss_function(predicted_label, label)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            #prediction = predicted_label#.argmax(1)#.item()\n",
    "            actual = label.reshape(-1)\n",
    "            \n",
    "            predicted_label = torch.argmax(predicted_label, dim=1 ) \n",
    "            accuracy += torch.eq(predicted_label, actual).sum().item()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "        train_loss = train_loss / len(train_dataloader)\n",
    "        accuracy = accuracy * 100.0 / len(dataset_train)\n",
    "\n",
    "        EPOCH_VAL_ACC, EPOCH_VAL_LOSS, F1_score = evaluate(val_dataloader, dataset_valid, model, loss_function)\n",
    "\n",
    "        print(f'Epoch: {epoch+1} | Train Loss: {train_loss} | Accuracy: {accuracy} | Val Accuracy: {EPOCH_VAL_ACC} | Val Loss: {EPOCH_VAL_LOSS} | F1 Score: {F1_score}')\n",
    "        train_loss_list.append(train_loss)\n",
    "        val_loss_list.append(EPOCH_VAL_LOSS)\n",
    "        train_accu_list.append(accuracy)\n",
    "        val_accu_list.append(EPOCH_VAL_ACC)\n",
    "        \n",
    "    return train_loss_list, val_loss_list, train_accu_list, val_accu_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(val_dataloader, dataset_valid, model, loss_function):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total_count = 0\n",
    "\n",
    "    # for f1 score\n",
    "    prediction_labels = []\n",
    "    actual_labels = []\n",
    "\n",
    "    val_loss = 0\n",
    "\n",
    "    with torch.no_grad():    \n",
    "        for text, label in val_dataloader:\n",
    "            text = text.to(device)\n",
    "            label = label.to(device)\n",
    "            \n",
    "            # feed the validation text into the model, and get the probabilities.\n",
    "            predicted_label = model(text)\n",
    "\n",
    "            # calculate loss\n",
    "            loss = loss_function(predicted_label, label)\n",
    "            \n",
    "            # validation accuracy\n",
    "            actual = label.reshape(-1)\n",
    "            predicted_label = torch.argmax(predicted_label, dim=1 ) \n",
    "            correct += torch.eq(predicted_label, actual).sum().item()\n",
    "\n",
    "            # to cal f1 score.\n",
    "            prediction_labels.append(predicted_label)\n",
    "            actual_labels.append(actual)   \n",
    "\n",
    "            # convert probabilities into 0/1.\n",
    "            #predicted_label = torch.round(predicted_label).type(torch.int64)\n",
    "            \n",
    "            # count the number of correctly predicted labels.\n",
    "            #correct += torch.eq(predicted_label, label).sum().item()\n",
    "            \n",
    "            # get the total length of the sentences in val_dataloader\n",
    "            #total_count += label.size(0)\n",
    "\n",
    "            val_loss += loss.item()\n",
    "        val_loss = val_loss / len(val_dataloader)\n",
    "\n",
    "        # convert unequal length of lists of tensors to on single tensors.\n",
    "        #actual_labels = torch.flatten(torch.stack(actual_labels)) \n",
    "        actual_labels = torch.cat(actual_labels).to('cpu')\n",
    "        #prediction_labels = torch.flatten(torch.stack(prediction_labels)) \n",
    "        prediction_labels = torch.cat(prediction_labels).to('cpu')\n",
    "        \n",
    "        F1_score = f1_score(actual_labels, prediction_labels)\n",
    "        \n",
    "    # returns the accuracy of the model\n",
    "    return correct * 100.0 / len(dataset_valid), val_loss, F1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_loss_list, val_loss_list, train_accu_list, val_accu_list = train(model, train_dataloader, val_dataloader, dataset_train, dataset_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = nn.CrossEntropyLoss()\n",
    "TEST_ACC, TEST_LOSS, F1_score = evaluate(val_dataloader, dataset_valid, model, loss_function = nn.CrossEntropyLoss())\n",
    "print(\"The test accuracy is: {:.2f}%\".format(TEST_ACC))\n",
    "print(\"F1 Score on Test data is: {:.2f}\".format(F1_score))\n",
    "print(\"Loss on Test Data is: {:.2f}\".format(TEST_LOSS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "fig, (ax1) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "fig.suptitle('Loss and accuracy for HAN Model.')\n",
    "\n",
    "epochs = 30 + 1\n",
    "\n",
    "# accuracy Plot\n",
    "train_accu, = ax1[0].plot(range(1, epochs), train_accu_list, label=\"Training Accuracy\")  \n",
    "val_accu, = ax1[0].plot(range(1, epochs), val_accu_list, label=\"Validation Accuracy\")  \n",
    "\n",
    "ax1[0].legend(handles=[train_accu, val_accu])\n",
    "ax1[0].set_xlabel(\"Epochs\")\n",
    "ax1[0].set_ylabel(\"Accuracy\")\n",
    "ax1[0].set_title(\"Accuracy for every Epochs\")\n",
    "ax1[0].set_xticks(range(1, epochs))\n",
    "\n",
    "train_loss, = ax1[1].plot(range(1, epochs), train_loss_list, label=\"Training Loss\")  \n",
    "val_loss, = ax1[1].plot(range(1, epochs), val_loss_list, label=\"Validation Loss\")  \n",
    "\n",
    "ax1[1].legend(handles=[train_loss, val_loss])\n",
    "ax1[1].set_xlabel(\"Epochs\")\n",
    "ax1[1].set_ylabel(\"Loss\")\n",
    "ax1[1].set_title(\"Loss for every Epochs\")\n",
    "ax1[1].set_xticks(range(1, epochs))\n",
    "\n",
    "# do not need the third plot.\n",
    "#fig.delaxes(ax2[1])\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
