{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This notebook contains minimum implementation of HAN model and training/evaluation functions.\n",
    "This notebook is/was not used for training/validating/inferencing/Visualizing for our research project.\n",
    "The implementation of HAN architecture works for different datasets as well.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = get_tokenizer('basic_english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CommentDataset(Dataset):\n",
    "    def __init__(self, folder_path, train=False, test=False, valid=False):\n",
    "        \n",
    "        if (train==False and test==False and valid==False):\n",
    "            raise Exception('One of the `train`, `test` or `valid` needs to be True, got `train = {}` `test = {}` and `valid = {}`'.format(train, test, valid))\n",
    "        if (train==True and test==True and valid == True):\n",
    "            raise Exception('Only one of the `train`, `test` or `valid` needs to be True, got `train = {}` `test = {}` and `valid = {}`'.format(train, test, valid))\n",
    "        if (train==True and test==True):\n",
    "            raise Exception('Only one of the `train` or `test` needs to be True, got `train = {}`, and `test = {}`'.format(train, test))\n",
    "        if (train==True and valid==True):\n",
    "            raise Exception('Only one of the `train` or `valid` needs to be True, got `train = {}`, and `valid = {}`'.format(train, valid))\n",
    "        if (test==True and valid==True):\n",
    "            raise Exception('Only one of the `test` or `valid` needs to be True, got `test = {}`, and `valid = {}`'.format(test, valid))\n",
    "\n",
    "        self.train_df = None\n",
    "        self.test_df = None\n",
    "        self.valid_df = None\n",
    "\n",
    "        # boolean values\n",
    "        self.train = train\n",
    "        self.test = test\n",
    "        self.valid = valid\n",
    "\n",
    "        self.data_selected = None\n",
    "        self.comment_selected = None\n",
    "    \n",
    "        self.train_comment = []\n",
    "        self.test_comment = []\n",
    "        self.val_comment = []\n",
    "    \n",
    "        # Read the dataset\n",
    "        self.data = pd.read_csv(folder_path, sep = \",\")#.head(20)\n",
    "        \n",
    "        self.data = shuffle(self.data)\n",
    "        self.data.reset_index(inplace=True, drop=True)\n",
    "        \n",
    "        self.data['Authors Biasness'].replace('LEFT', 0, inplace=True)\n",
    "        self.data['Authors Biasness'].replace('RIGHT', 1, inplace=True)\n",
    "        \n",
    "        # split the dataset into train, test, and valid.\n",
    "        self.train_df, test_df = train_test_split(self.data, test_size=0.2,  random_state=11)\n",
    "        self.test_df, self.valid_df = train_test_split(test_df, test_size=0.5,  random_state=96)\n",
    "        \n",
    "        # a basic preprocessor, beeds to be done  outside the dataset function.\n",
    "        #idx = [4, 7, 12, 19]\n",
    "        #list_d = []\n",
    "        #for i in range(len(self.data[\"comment\"])):\n",
    "        #    if i not in idx:\n",
    "        #        d = self.data[\"comment\"][i].split(\"', \")\n",
    "        #        list_d.append(d)\n",
    "        #    else:\n",
    "        #        d = self.data[\"comment\"][i].split(\"\\\", \")\n",
    "        #        list_d.append(d)\n",
    "        \n",
    "        if self.train == True:\n",
    "            # do the sorting\n",
    "            # Sort the dataframe according to the number of comments on documents.\n",
    "            self.train_df.sort_values(by=['Num of Comments'], ascending=False, inplace=True)       \n",
    "            comments = []\n",
    "            for com in self.train_df[\"Authors Comment\"]:\n",
    "                comments.append(com.split(\"-|-\")[:-1])\n",
    "            self.train_comment = comments\n",
    "        elif self.test == True:\n",
    "            # no need to sort\n",
    "            comments = []\n",
    "            for com in self.test_df[\"Authors Comment\"]:\n",
    "                comments.append(com.split(\"-|-\")[:-1])\n",
    "            self.test_comment = comments\n",
    "        elif self.valid == True:\n",
    "            # no need to sort\n",
    "            comments = []\n",
    "            for com in self.valid_df[\"Authors Comment\"]:\n",
    "                comments.append(com.split(\"-|-\")[:-1])\n",
    "            self.val_comment = comments\n",
    "        \n",
    "        \n",
    "        # split the dataset into train, test, and valid.\n",
    "        #self.train_df, test_df = train_test_split(self.data, test_size=0.2,  random_state=11)\n",
    "        #self.test_df, self.valid_df = train_test_split(test_df, test_size=0.5,  random_state=96)\n",
    "\n",
    "        #if self.train == True:\n",
    "        # do the sorting\n",
    "        #self.train_df[\"combined_text_len\"] = 0\n",
    "        #for i, row in self.train_df.iterrows():\n",
    "        #    self.train_df.at[i, \"combined_text_len\"] = len(tokenizer(row[\"combined_text\"]))\n",
    "        # Sort the dataframe according to the length of the sentence\n",
    "        #self.train_df.sort_values(by=['combined_text_len'], ascending=False, inplace=True)            \n",
    "        #elif self.test == True:\n",
    "            # no need ot sort\n",
    "        #    self.test_df = self.test_df\n",
    "        #elif self.valid == True:\n",
    "            # no need ot sort\n",
    "        #    self.valid_df = self.valid_df\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.train == True:\n",
    "            self.data_selected = self.train_df\n",
    "            self.comment_selected = self.train_comment\n",
    "\n",
    "        elif self.test == True:\n",
    "            self.data_selected = self.test_df\n",
    "            self.comment_selected = self.test_comment\n",
    "\n",
    "        elif self.valid == True:\n",
    "            self.data_selected = self.valid_df\n",
    "            self.comment_selected = self.val_comment\n",
    "\n",
    "        label = self.data_selected.iloc[idx][\"Authors Biasness\"]\n",
    "        sentence = self.comment_selected[idx]\n",
    "        return sentence, label\n",
    "\n",
    "    def __len__(self):\n",
    "        \n",
    "        \n",
    "        if self.train == True:\n",
    "            len_ = len(self.train_comment)\n",
    "\n",
    "        elif self.test == True:\n",
    "            len_ = len(self.test_comment)\n",
    "\n",
    "        elif self.valid == True:\n",
    "            len_ = len(self.val_comment)\n",
    "        \n",
    "        \n",
    "        #if self.train == True:\n",
    "        #len_ = len(self.comment)\n",
    "\n",
    "        #elif self.test == True:\n",
    "        #    len_ = len(self.test_df)\n",
    "\n",
    "        #elif self.valid == True:\n",
    "        #    len_ = len(self.valid_df)\n",
    "        return len_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = \"16. Training Dataset revisit.csv\"\n",
    "\n",
    "# create dataset loader for train and evaluation set.\n",
    "dataset_train = CommentDataset(data_folder, train=True, test=False, valid=False)\n",
    "dataset_valid = CommentDataset(data_folder, train=False, test=False, valid=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yield_tokens(data_iter):\n",
    "    for iter_, _ in data_iter:\n",
    "        for sentence in iter_:\n",
    "            yield tokenizer(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create vocabulary from the training data.\n",
    "vocab = build_vocab_from_iterator(yield_tokens(dataset_train), specials=[\"<unk>\"])\n",
    "vocab.set_default_index(vocab[\"<unk>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(vocab.get_itos()) # len(vocab.get_stoi()) - length of the vocabulary\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_pipeline = lambda x: vocab(tokenizer(x))\n",
    "label_pipeline = lambda x: int(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# works perfectly. backup\n",
    "def collate_batch(batch):\n",
    "    label_list, text_list = [], []\n",
    "    for (_text, _label) in batch:\n",
    "        \n",
    "        label_list.append(torch.tensor(label_pipeline(_label), dtype=torch.int64 ))\n",
    "        \n",
    "        texts = []\n",
    "        for t in _text:\n",
    "            texts.append(torch.tensor(text_pipeline(t), dtype=torch.int64))\n",
    "        text_list.append(texts)\n",
    "    \n",
    "    sentence_length, word_length = get_max_length(text_list)\n",
    "    \n",
    "    text_list_p = []\n",
    "    for t in text_list:\n",
    "        # input shape: a list of tensors with unequal length of sentences.\n",
    "        # padding to the highest length of the sequence.\n",
    "        p = [ torch.cat((batch, torch.LongTensor([vocab_size-1]).repeat(word_length - len(batch))), dim=0) \n",
    "                if((word_length - batch.shape[0]) !=  0 ) else batch for batch in t]\n",
    "        \n",
    "        # input shape: a list of tensors with unequal length of documents.\n",
    "        # padding to the highest length of the document.\n",
    "        if(sentence_length - len(p)) !=  0:\n",
    "            extended_sentences = [torch.LongTensor([vocab_size-1 for _ in range(word_length)] )\n",
    "                                  for _ in range(sentence_length - len(p))]\n",
    "            p.extend(extended_sentences)\n",
    "\n",
    "            #p = pad_sequence(text_list[0], batch_first=False, padding_value = vocab_size-1)\n",
    "            #  OUTPUT shape: [MAX_LENGTH_OF_THE_SENTENCE_IN_BATCH, NUM_SENTENCES] => [57, 5]\n",
    "        \n",
    "        p = torch.stack(p)\n",
    "        # OUTPUT shape: [NUM_SENTENCES X MAX_LENGTH_OF_THE_SENTENCE_IN_BATCH] => [5,57]\n",
    "        text_list_p.append(p) # for every batch\n",
    "    \n",
    "    text_list_p = torch.stack(text_list_p)\n",
    "    # OUTPUT shape: [BATCH_SIZE X NUM_SENTENCES X MAX_LENGTH_OF_THE_SENTENCE_IN_DOCUMENT ] => [3, 5, 57]\n",
    "\n",
    "    #text_list_p = torch.permute(text_list_p, (2, 1, 0))\n",
    "    # NOt sure, whether it should be this: OUTPUT shape: [MAX_LENGTH_OF_THE_SENTENCE_IN_BATCH X NUM_SENTENCES X BATCH_SIZE] => [57, 5, 2]\n",
    "    \n",
    "    # convert a list of tensors to tensors.\n",
    "    # input : a list of tensors of len BATCH_SIZE\n",
    "    label_list = torch.stack(label_list)   \n",
    "    # OUTPUT shape: [BATCH_SIZE]\n",
    "    \n",
    "    return text_list_p, label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_length(doc):\n",
    "    \"\"\"\n",
    "    doc = [\n",
    "        [\n",
    "                [1,2,3,4,5],\n",
    "               [1,2,3,4],\n",
    "               [1,2,3,4,5,6,7,8],\n",
    "               [1,2,3,4,5]\n",
    "        ], \n",
    "        [\n",
    "                [1,2],\n",
    "               [1,2,3,4,5,6,7,8,9],\n",
    "               [1,2,3,4,5],\n",
    "               [1,2,3,4],\n",
    "                [1, 2,3,4,5,6]\n",
    "        ]\n",
    "    ]\n",
    "\n",
    "    #sentence_in_doc, word_in_sentence = get_max_length(doc)\n",
    "    sentence_in_doc -> 5, and word_in_sentence -> 9\n",
    "    \"\"\"\n",
    "    \n",
    "    sent_length_list = []\n",
    "    word_length_list = []\n",
    "\n",
    "    for sent in doc:\n",
    "        sent_length_list.append(len(sent))\n",
    "\n",
    "        for word in sent:\n",
    "            word_length_list.append(len(word))\n",
    "\n",
    "    sorted_word_length = sorted(word_length_list)\n",
    "    sorted_sent_length = sorted(sent_length_list)\n",
    "    \n",
    "    #return sorted_sent_length[int(0.8*len(sorted_sent_length))], sorted_word_length[int(0.8*len(sorted_word_length))]\n",
    "    return sorted_sent_length[-1], sorted_word_length[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Download_and_extract():\n",
    "    print(\"This might take some time...\")\n",
    "    print(\"Downloading...\")\n",
    "    os.system('wget https://nlp.stanford.edu/data/glove.840B.300d.zip')\n",
    "    \n",
    "    Extract()\n",
    "    \n",
    "def Extract():\n",
    "    print(\"Extracting...\")\n",
    "    # extract and save to the same directory.\n",
    "    with zipfile.ZipFile('glove.840B.300d.zip', 'r') as zip_ref:\n",
    "        zip_ref.extractall(\"./\")\n",
    "    print(\"Done!\")\n",
    "    \n",
    "def load_pretrained_embedding_matrix():\n",
    "    # Downloadin Glove word vector\n",
    "    # this might take some time........... ~5 mins.\n",
    "    if((os.path.isfile('glove.840B.300d.zip') == False)):\n",
    "        Download_and_extract()\n",
    "    elif((os.path.isfile('glove.840B.300d.zip') == True) and (os.path.isfile('glove.840B.300d.txt') == False)):\n",
    "         Extract()\n",
    "    else:\n",
    "        print(\"Already Downloaded and extracted!\")\n",
    "\n",
    "    #!wget https://nlp.stanford.edu/data/glove.840B.300d.zip\n",
    "    #!unzip glove.840B.300d.zip\n",
    "\n",
    "# https://github.com/MohammadWasil/Visual-Question-Answering-VQA/blob/master/2.%20Dataset%20Used%20in%20Training..ipynb\n",
    "def GloveModel(file_path, vocab):\n",
    "    embedding_index = {}\n",
    "    f = open(file_path,'r', encoding='utf8')\n",
    "    embedding_index = {}\n",
    "    print(\"Opened!\")\n",
    "\n",
    "    for j, line in enumerate(f):\n",
    "        splitLine = line.split(' ')\n",
    "        word = splitLine[0]\n",
    "        embedding = np.asarray(splitLine[1:], dtype='float32')\n",
    "        embedding_index[word] = embedding\n",
    "      \n",
    "    print(\"Done.\",len(embedding_index),\" words loaded!\")\n",
    "    EMBEDDING_DIM = 300\n",
    "    embedding_matrix = np.zeros((len(vocab.get_stoi()) + 1, EMBEDDING_DIM))\n",
    "    print(embedding_matrix.shape)\n",
    "\n",
    "    for index, word in enumerate(vocab.get_itos()):\n",
    "        embedding_vector = embedding_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "          # words not found in embedding index will be all-zeros.\n",
    "          embedding_matrix[index] = embedding_vector\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# can change this accordingly.\n",
    "USE_PRETRAINED_EMBEDDING_MATRIX = False\n",
    "\n",
    "if USE_PRETRAINED_EMBEDDING_MATRIX:\n",
    "    # download an dextract the glove embedding if they're not.\n",
    "    load_pretrained_embedding_matrix()\n",
    "    # load the embedding matrix.\n",
    "    embedding_matrix = GloveModel(\"glove.840B.300d.txt\", vocab)\n",
    "else:\n",
    "    embedding_matrix = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "train_dataloader = DataLoader(dataset_train, batch_size=BATCH_SIZE,\n",
    "                            shuffle=False, collate_fn=collate_batch)\n",
    "val_dataloader = DataLoader(dataset_valid,\n",
    "                            shuffle=False, collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "\n",
    "    def __init__(self, VOCAB_SIZE, EMBEDDING_DIMENSION, num_class, ENCODER_HIDDEN_DIMENSION, DECODER_HIDDEN_DIMENSION, USE_PRETRAINED_EMBEDDING_MATRIX, embedding_matrix):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.vocab_size = VOCAB_SIZE\n",
    "        self.embed_dim = EMBEDDING_DIMENSION\n",
    "        \n",
    "        self.encoder_hidden_dim = ENCODER_HIDDEN_DIMENSION\n",
    "        self.decoder_hidden_dim = DECODER_HIDDEN_DIMENSION\n",
    "        \n",
    "        self.num_class = num_class\n",
    "\n",
    "        if USE_PRETRAINED_EMBEDDING_MATRIX:\n",
    "            self.vocab_size = embedding_matrix.shape[0]\n",
    "            self.embed_dim = embedding_matrix.shape[1]\n",
    "            \n",
    "            self.embedding = nn.Embedding(num_embeddings = self.vocab_size, embedding_dim = self.embed_dim)\n",
    "            self.embedding.weight=nn.Parameter(torch.tensor(embedding_matrix, dtype=torch.float32))\n",
    "        else:\n",
    "            self.embedding = nn.Embedding(self.vocab_size, self.embed_dim)\n",
    "        \n",
    "        self.gru = nn.GRU(self.embed_dim, self.encoder_hidden_dim, bidirectional =True)\n",
    "\n",
    "        self.attention = Attention(self.encoder_hidden_dim*2)\n",
    "        \n",
    "    def forward(self, text):\n",
    "        \n",
    "        # input shape: [MAX_LENGTH_OF_THE_SENTENCE_IN_BATCH X NUM_SENTENCES X BATCH_SIZE] \n",
    "        # or input shape: [MAX_LENGTH_OF_THE_SENTENCE_IN_BATCH X NUM_SENTENCES] \n",
    "        embedded = self.embedding(text)\n",
    "        # output shape: [MAX_LENGTH_OF_THE_SENTENCE_IN_BATCH X NUM_SENTENCES X EMBEDDING_DIMENSION]\n",
    "        # 2nd output: [BATCH_SIZE, MAX_LENGTH_OF_THE_SENTENCE_IN_DOC X 100]\n",
    "        \n",
    "        # input shape: [MAX_LENGTH_OF_THE_SENTENCE_IN_BATCH X NUM_SENTENCES x EMBEDDING_DIMENSION]\n",
    "        gru_out, hidden = self.gru(embedded)\n",
    "        # gru_out shape: [MAX_LENGTH_OF_THE_SENTENCE_IN_BATCH, NUM_SENTENCES, ENCODER_HIDDEN_DIMENSION*2]\n",
    "        # hidden[0] shape: [1, NUM_SENTENCES, ENCODER_HIDDEN_DIMENSION]\n",
    "        # hidden[1] shape: [1, NUM_SENTENCES, ENCODER_HIDDEN_DIMENSION]\n",
    "        \n",
    "        # 2nd gru_out shape: [BATCH_SIZE, MAX_LENGTH_OF_THE_SENTENCE_IN_DOC, ENCODER_HIDDEN_DIMENSION*2]\n",
    "        # 2nd hidden shape: [BATCH_SIZE, MAX_LENGTH_OF_THE_SENTENCE_IN_DOC, ENCODER_HIDDEN_DIMENSION]\n",
    "        \n",
    "        # concatenate both forward and backward hidden vectors\n",
    "        #hidden_f_b = torch.cat((hidden[0,:,:], hidden[1,:,:]), dim = 1)\n",
    "        # output shape: [NUM_SENTENCES, ENCODER_HIDDEN_DIMENSION*2]\n",
    "        \n",
    "\n",
    "        alpha, s_i = self.attention(gru_out) # fome the diagram, it is s_i.\n",
    "        \n",
    "        return alpha, s_i, gru_out\n",
    "\n",
    "class Sentence_Encoder(nn.Module):\n",
    "\n",
    "    def __init__(self, VOCAB_SIZE, EMBEDDING_DIMENSION, num_class, ENCODER_HIDDEN_DIMENSION, DECODER_HIDDEN_DIMENSION):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.vocab_size = VOCAB_SIZE\n",
    "        self.embed_dim = EMBEDDING_DIMENSION\n",
    "        \n",
    "        self.encoder_hidden_dim = ENCODER_HIDDEN_DIMENSION\n",
    "        self.decoder_hidden_dim = DECODER_HIDDEN_DIMENSION\n",
    "        \n",
    "        self.num_class = num_class\n",
    "        \n",
    "        #self.embedding = nn.Embedding(self.vocab_size, self.embed_dim)\n",
    "        \n",
    "        self.gru = nn.GRU(self.encoder_hidden_dim*2, self.encoder_hidden_dim, bidirectional =True)\n",
    "\n",
    "        self.attention = Attention(self.encoder_hidden_dim*2)\n",
    "\n",
    "        #self.init_weights()\n",
    "        \n",
    "    def forward(self, word_embed):\n",
    "        # input shape: [BATCH X NUM_SENTENCES x EMBEDDING_DIMENSION*2]\n",
    "        gru_out, hidden = self.gru(word_embed)\n",
    "        # gru_out shape: [BATCH X NUM_SENTENCES x EMBEDDING_DIMENSION*2]\n",
    "        # hidden shape: [BATCH X NUM_SENTENCES x EMBEDDING_DIMENSION]\n",
    "        \n",
    "        # concatenate both forward and backward hidden vectors\n",
    "        #hidden_f_b = torch.cat((hidden[0,:,:], hidden[1,:,:]), dim = 1)\n",
    "        # output shape: [NUM_SENTENCES, ENCODER_HIDDEN_DIMENSION*2]\n",
    "        \n",
    "        alpha, v = self.attention(gru_out) # from the diagram, it is v.\n",
    "        # output: [BATCH X 1 X ENCODER_HIDDEN_DIMENSION*2]\n",
    "        return alpha, v, gru_out\n",
    "      \n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, ENCODER_HIDDEN_DIMENSION):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder_hidden_dim = ENCODER_HIDDEN_DIMENSION\n",
    "        \n",
    "        self.linear = nn.Linear(self.encoder_hidden_dim, self.encoder_hidden_dim)\n",
    "        self.context = nn.Linear(self.encoder_hidden_dim, 1)\n",
    "        \n",
    "    def forward(self, gru_out):\n",
    "        \n",
    "        # input: [MAX_LENGTH_OF_THE_SENTENCE_IN_BATCH, NUM_SENTENCES, ENCODER_HIDDEN_DIMENSION*2]\n",
    "        hidden_enc = self.linear(gru_out)\n",
    "        # output: [MAX_LENGTH_OF_THE_SENTENCE_IN_BATCH, NUM_SENTENCES, ENCODER_HIDDEN_DIMENSION*2]\n",
    "\n",
    "        # 2nd output shape: [BATCH_SIZE, MAX_LENGTH_OF_THE_SENTENCE_IN_DOC, ENCODER_HIDDEN_DIMENSION*2]\n",
    "    \n",
    "        \n",
    "        # input: [MAX_LENGTH_OF_THE_SENTENCE_IN_BATCH, NUM_SENTENCES, ENCODER_HIDDEN_DIMENSION*2]\n",
    "        u = torch.tanh(hidden_enc)\n",
    "        # output: [MAX_LENGTH_OF_THE_SENTENCE_IN_BATCH, NUM_SENTENCES, ENCODER_HIDDEN_DIMENSION*2]\n",
    "        \n",
    "        # 2nd output shape: [BATCH_SIZE, MAX_LENGTH_OF_THE_SENTENCE_IN_DOC, ENCODER_HIDDEN_DIMENSION*2]\n",
    "        \n",
    "        # input: [MAX_LENGTH_OF_THE_SENTENCE_IN_BATCH, NUM_SENTENCES, ENCODER_HIDDEN_DIMENSION*2]\n",
    "        context_vector = self.context(u)\n",
    "        # output: [MAX_LENGTH_OF_THE_SENTENCE_IN_BATCH, NUM_SENTENCES, 1]\n",
    "        \n",
    "        # 2nd output shape: [BATCH_SIZE, MAX_LENGTH_OF_THE_SENTENCE_IN_DOC, 1]\n",
    "        \n",
    "        # input: [MAX_LENGTH_OF_THE_SENTENCE_IN_BATCH, NUM_SENTENCES, 1]\n",
    "        alpha = F.softmax(context_vector, dim=1)   # this needs to be send also##################\n",
    "        # output: [MAX_LENGTH_OF_THE_SENTENCE_IN_BATCH, NUM_SENTENCES, 1]\n",
    "        \n",
    "        # 2nd output shape: [BATCH_SIZE, MAX_LENGTH_OF_THE_SENTENCE_IN_DOC, 1]\n",
    "        \n",
    "        alpha=alpha.permute(0, 2, 1)\n",
    "        # 2nd output shape: [BATCH_SIZE, 1, MAX_LENGTH_OF_THE_SENTENCE_IN_DOC]\n",
    "        \n",
    "        a = alpha@gru_out  \n",
    "        # 2nd output shape: [BATCH_SIZE, 1, ENCODER_HIDDEN_DIMENSION*2]\n",
    "        return alpha, a\n",
    "    \n",
    "class HierarchicalAttentionNetwork(nn.Module):\n",
    "\n",
    "    def __init__(self, VOCAB_SIZE, EMBEDDING_DIMENSION, num_class, ENCODER_HIDDEN_DIMENSION, DECODER_HIDDEN_DIMENSION, USE_PRETRAINED_EMBEDDING_MATRIX, embedding_matrix, device):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.vocab_size = VOCAB_SIZE\n",
    "        self.embedding_size = EMBEDDING_DIMENSION\n",
    "        self.num_class = num_class\n",
    "        self.ENCODER_HIDDEN_DIMENSION = ENCODER_HIDDEN_DIMENSION\n",
    "        self.DECODER_HIDDEN_DIMENSION = DECODER_HIDDEN_DIMENSION\n",
    "        self.embedding_matrix = embedding_matrix\n",
    "        self.USE_PRETRAINED_EMBEDDING_MATRIX = USE_PRETRAINED_EMBEDDING_MATRIX\n",
    "\n",
    "        self.model = Encoder(self.vocab_size, self.embedding_size, self.num_class, self.ENCODER_HIDDEN_DIMENSION, self.DECODER_HIDDEN_DIMENSION, self.USE_PRETRAINED_EMBEDDING_MATRIX, self.embedding_matrix).to(device)\n",
    "        self.sent_model = Sentence_Encoder(self.vocab_size, self.embedding_size, self.num_class, self.ENCODER_HIDDEN_DIMENSION, self.DECODER_HIDDEN_DIMENSION).to(device)\n",
    "        \n",
    "        self.linear = nn.Linear(self.ENCODER_HIDDEN_DIMENSION*2, self.num_class)\n",
    "        \n",
    "    def forward(self, text):\n",
    "        \n",
    "        text = text.permute(1, 0, 2)\n",
    "        word_a_list, word_s_list = [], []\n",
    "\n",
    "        # Iterate through all the sentences in every batch\n",
    "        for sent in text:\n",
    "\n",
    "            # input: [BATCH_SIZE, MAX_LENGTH_OF_THE_SENTENCE_IN_DOC]\n",
    "            alpha_word, word_s, gru_out = self.model(sent)\n",
    "            # output: word_s: [BATCH_SIZE, 1, ENCODER_HIDDEN_DIMENSION*2]\n",
    "\n",
    "            word_a_list.append(alpha_word)\n",
    "            word_s_list.append(word_s)\n",
    "\n",
    "        word_s_list = torch.cat(word_s_list, dim=1)\n",
    "        # output: word_s: [BATCH_SIZE, NUM_SENTENCES, ENCODER_HIDDEN_DIMENSION*2]\n",
    "        \n",
    "        alpha_sentence, v, gru_out_sentence = self.sent_model(word_s_list)\n",
    "        # output v: # output: [BATCH X 1 X ENCODER_HIDDEN_DIMENSION*2]\n",
    "        # output gru_out_sentence: [BATCH X NUM_SENTENCES x EMBEDDING_DIMENSION*2]\n",
    "        \n",
    "        v_output = self.linear(v)\n",
    "        # v_output shape: [BATCH, 1, Num_classes]        \n",
    "        \n",
    "        classifier = F.softmax(v_output, dim=2).squeeze(1)\n",
    "        # classifier shape: [BATCH, 1, Num_classes]        \n",
    "        \n",
    "        return classifier, word_a_list, alpha_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from datetime import datetime\n",
    "def save_model(epoch, model, optimizer, train_loss_list, val_loss_list, train_accu_list, val_accu_list, path):\n",
    "    # path in .pth or pt format\n",
    "    #now = datetime.now()\n",
    "    #current_time = now.strftime(\"%d/%m/%Y %H:%M:%S\")\n",
    "    \n",
    "    # save the file\n",
    "    torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'train_loss_list': train_loss_list,\n",
    "                'val_loss_list': val_loss_list,\n",
    "                'train_accu_list': train_accu_list,\n",
    "                'val_accu_list': val_accu_list,\n",
    "                }, path.format(epoch) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_class = 2\n",
    "embedding_size = 100\n",
    "ENCODER_HIDDEN_DIMENSION = 64\n",
    "DECODER_HIDDEN_DIMENSION = 32\n",
    "\n",
    "han_model = HierarchicalAttentionNetwork(vocab_size, embedding_size, num_class, ENCODER_HIDDEN_DIMENSION, DECODER_HIDDEN_DIMENSION).to(device)\n",
    "\n",
    "def train(han_model, train_dataloader, val_dataloader, dataset_train, dataset_valid):\n",
    "    optimizer = Adam(han_model.parameters(), 0.0001)\n",
    "    loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "    train_loss_list = []\n",
    "    val_loss_list = []\n",
    "    train_accu_list = []\n",
    "    val_accu_list = []\n",
    "\n",
    "    for epoch in range(0, 20):\n",
    "        han_model.train()\n",
    "        han_model.to(device)\n",
    "        train_loss = 0\n",
    "        \n",
    "        accuracy = 0\n",
    "        \n",
    "        for idx, (text, label) in enumerate(train_dataloader):\n",
    "            text = text.to(device)\n",
    "            label = label.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            predicted_label, alpha_word, alpha_sentence = han_model(text)\n",
    "\n",
    "            loss = loss_function(predicted_label, label)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            #prediction = predicted_label#.argmax(1)#.item()\n",
    "            actual = label.reshape(-1)\n",
    "            \n",
    "            predicted_label = torch.argmax(predicted_label, dim=1 ) \n",
    "            accuracy += torch.eq(predicted_label, actual).sum().item()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "        train_loss = train_loss / len(train_dataloader)\n",
    "        accuracy = accuracy * 100.0 / len(dataset_train)\n",
    "\n",
    "        EPOCH_VAL_ACC, EPOCH_VAL_LOSS, F1_score = evaluate(val_dataloader, han_model, dataset_valid, loss_function)\n",
    "\n",
    "        print(f'Epoch: {epoch+1} | Train Loss: {train_loss} | Accuracy: {accuracy} | Val Accuracy: {EPOCH_VAL_ACC} | Val Loss: {EPOCH_VAL_LOSS} | F1 Score: {F1_score}')\n",
    "        train_loss_list.append(train_loss)\n",
    "        val_loss_list.append(EPOCH_VAL_LOSS)\n",
    "        train_accu_list.append(accuracy)\n",
    "        val_accu_list.append(EPOCH_VAL_ACC)\n",
    "        \n",
    "        # save the model.\n",
    "        save_model(epoch, model, optimizer, train_loss_list, val_loss_list, train_accu_list, val_accu_list, path)\n",
    "        \n",
    "    return train_loss_list, val_loss_list, train_accu_list, val_accu_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(val_dataloader, model, dataset_valid, loss_function):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total_count = 0\n",
    "\n",
    "    # for f1 score\n",
    "    prediction_labels = []\n",
    "    actual_labels = []\n",
    "\n",
    "    val_loss = 0\n",
    "\n",
    "    with torch.no_grad():    \n",
    "        for text, label in val_dataloader:\n",
    "            text = text.to(device)\n",
    "            label = label.to(device)\n",
    "            \n",
    "            # feed the validation text into the model, and get the probabilities.\n",
    "            predicted_label, alpha_word, alpha_sentence = model(text)\n",
    "\n",
    "            # calculate loss\n",
    "            loss = loss_function(predicted_label, label)\n",
    "            \n",
    "            # validation accuracy\n",
    "            actual = label.reshape(-1)\n",
    "            predicted_label = torch.argmax(predicted_label, dim=1 ) \n",
    "            correct += torch.eq(predicted_label, actual).sum().item()\n",
    "\n",
    "            # to cal f1 score.\n",
    "            prediction_labels.append(predicted_label)\n",
    "            actual_labels.append(actual)   \n",
    "\n",
    "            # convert probabilities into 0/1.\n",
    "            #predicted_label = torch.round(predicted_label).type(torch.int64)\n",
    "            \n",
    "            # count the number of correctly predicted labels.\n",
    "            #correct += torch.eq(predicted_label, label).sum().item()\n",
    "            \n",
    "            # get the total length of the sentences in val_dataloader\n",
    "            #total_count += label.size(0)\n",
    "\n",
    "            val_loss += loss.item()\n",
    "        val_loss = val_loss / len(val_dataloader)\n",
    "\n",
    "        # convert unequal length of lists of tensors to on single tensors.\n",
    "        #actual_labels = torch.flatten(torch.stack(actual_labels)) \n",
    "        actual_labels = torch.cat(actual_labels).to('cpu')\n",
    "        #prediction_labels = torch.flatten(torch.stack(prediction_labels)) \n",
    "        prediction_labels = torch.cat(prediction_labels).to('cpu')\n",
    "\n",
    "        F1_score = f1_score(actual_labels, prediction_labels)\n",
    "        \n",
    "    \n",
    "    # returns the accuracy of the model\n",
    "    return correct * 100.0 / len(dataset_valid), val_loss, F1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss_list, val_loss_list, train_accu_list, val_accu_list = train(han_model, train_dataloader, val_dataloader, dataset_train, dataset_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = nn.CrossEntropyLoss()\n",
    "TEST_ACC, TEST_LOSS, F1_score = evaluate(test_dataloader, han_model, loss_function = nn.CrossEntropyLoss())\n",
    "print(\"The test accuracy is: {:.2f}%\".format(TEST_ACC))\n",
    "print(\"F1 Score on Test data is: {:.2f}\".format(F1_score))\n",
    "print(\"Loss on Test Data is: {:.2f}\".format(TEST_LOSS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "fig, (ax1) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "fig.suptitle('Loss and accuracy for HAN Model.')\n",
    "\n",
    "# accuracy Plot\n",
    "train_accu, = ax1[0].plot(range(1, 21), train_accu_list, label=\"Training Accuracy\")  \n",
    "val_accu, = ax1[0].plot(range(1, 21), val_accu_list, label=\"Validation Accuracy\")  \n",
    "\n",
    "ax1[0].legend(handles=[train_accu, val_accu])\n",
    "ax1[0].set_xlabel(\"Epochs\")\n",
    "ax1[0].set_ylabel(\"Accuracy\")\n",
    "ax1[0].set_title(\"Accuracy for every Epochs\")\n",
    "ax1[0].set_xticks(range(1, 21))\n",
    "\n",
    "train_loss, = ax1[1].plot(range(1, 21), train_loss_list, label=\"Training Loss\")  \n",
    "val_loss, = ax1[1].plot(range(1, 21), val_loss_list, label=\"Validation Loss\")  \n",
    "\n",
    "ax1[1].legend(handles=[train_loss, val_loss])\n",
    "ax1[1].set_xlabel(\"Epochs\")\n",
    "ax1[1].set_ylabel(\"Loss\")\n",
    "ax1[1].set_title(\"Loss for every Epochs\")\n",
    "ax1[1].set_xticks(range(1, 21))\n",
    "\n",
    "# do not need the third plot.\n",
    "#fig.delaxes(ax2[1])\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
